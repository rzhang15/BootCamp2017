{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating Autoregressive Processes\n",
    "### by [Jason DeBacker](http://jasondebacker.com), July 2017\n",
    "This Jupyter notebook was written using Python 3.6. To execute all the code here, one will need a the Python script `ar1_approx.py` to be located in a directory with the relative path `..\\Code\\`.\n",
    "\n",
    "Consider the AR(1) process given by:\n",
    "\n",
    "$$ ln(z_{t+1}) = \\rho ln(z_{t}) + (1-\\rho)\\mu + \\varepsilon_{t}, $$\n",
    "\n",
    "where $\\varepsilon_{t}\\sim N(0,\\sigma_{\\varepsilon})$.  \n",
    "\n",
    "With such a process, $z$ can take on any value on the real line.  The distribution of $z$ looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# to print plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# set out parameters\n",
    "rho = 0.8\n",
    "mu = 0.0\n",
    "sigma_eps = 0.2\n",
    "\n",
    "# draw our shocks\n",
    "num_draws = 100000 # number of shocks to draw\n",
    "eps = np.random.normal(0.0, sigma_eps, size=(num_draws))\n",
    "\n",
    "# Compute z\n",
    "z = np.empty(num_draws)\n",
    "z[0] = 0.0 + eps[0]\n",
    "for i in range(1, num_draws):\n",
    "    z[i] = rho * z[i - 1] + (1 - rho) * mu + eps[i]\n",
    "    \n",
    "\n",
    "# plot distribution of z\n",
    "# sns.distplot(z, hist=False)\n",
    "sns.kdeplot(np.array(z), bw=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of $z$ should have variance = $\\sigma_{z}^{2} = \\frac{\\sigma_{\\epsilon}^{2}}{(1-\\rho)}$.  Let's check: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# theory says:\n",
    "sigma_z = sigma_eps / ((1 - rho ** 2) ** (1 / 2))\n",
    "print('Theoretical sigma_z = ', sigma_z)\n",
    "\n",
    "# from our simulation:\n",
    "sigma_z_simul = z.std()\n",
    "print('Simulated sigma_z = ', sigma_z_simul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close.  And it will get closer as you increae `N`.  \n",
    "\n",
    "For the computational solution to our dynamic programming problem, we'll want to approximate this continuous distribution with something over a finite grid.  There are a number of methods to do this (see Adda and Cooper (2003, pp. 56-59), Tauchen and Hussey (1991), and Rouwenhorst (1995) for some approaches).  What all of these approaches have in common is that they approximate the autoregressive process with a Markov process (technically a Markov chain, since it's over a finite grid). Generally, the approach is to divide the real line over which $z$ is defined into a finite set of intervals.  These approaches then compute the probabilities of transitioning between one interval and another to match the autoregressive processs.\n",
    "\n",
    "## 1. Adda-Cooper (2003) method\n",
    "\n",
    "Let us illustrate the general approach of these approximation methods through the Adda and Cooper (2003, pp. 56-59) (henceforth AC) method.  \n",
    "\n",
    "AC start by dividing the real line into intervals.  In particular, they divide the real line into intervals such that $z_{t}$ has an equal probability of falling into any of the intervals.  Since $\\varepsilon$ is distributed normally, the cut-off points given by $\\left\\{z^{i}\\right\\}_{i=0}^{N}$ for these intervals are defined by:\n",
    "\n",
    "$$ \\Phi\\left(\\frac{z^{i+1}-\\mu}{\\sigma_{z}}\\right) - \\Phi\\left(\\frac{z^{i}-\\mu}{\\sigma_{z}}\\right) = \\frac{1}{N}, $$\n",
    "\n",
    "where $N$ are the number of grid points in our discretized grid space and $\\Phi(\\cdot)$ is the cumulative density function of the standard normal distribution.  Solving this recursively we find that each cut-off point is given by:\n",
    "\n",
    "$$ z^{i} = \\sigma_{z} \\Phi^{-1}\\left(\\frac{i-1}{N}\\right) + \\mu $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Compute cut-off values\n",
    "N = 5  # number of grid points (will have one more cut-off point than this)\n",
    "z_cutoffs = (sigma_z * norm.ppf(np.arange(N + 1) / N)) + mu\n",
    "print('Cut-off values = ', z_cutoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the intervals through these cut-off values, we'll next want to compute the average value of $z$ in each of these intervals.  These values will be our grid points.  Let $z_{p}^{i}$ be grid point $i$ and define $z_{p}^{i}$ as the mean value of $z_{t}$ conditional on $z_{t}\\in\\left[z^{i},z^{i+1}\\right]$.  That is:\n",
    "\n",
    "$$ z_{p}^{i} = E\\left(z_{t}|z_{t}\\in \\left[z^{i},z^{i+1}\\right] \\right) = \\sigma_{z} \\frac{\\phi((z^{i}-\\mu)/\\sigma_z) - \\phi((z^{i+1}-\\mu)/\\sigma_z)}{\\Phi((z^{i+1}-\\mu)/\\sigma_z) - \\Phi((z^{i}-\\mu)/\\sigma_z)} + \\mu $$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$ z_{p}^{i} = N \\sigma_z \\left(\\phi\\left(\\frac{z^{i}-\\mu}{\\sigma_z}\\right) - \\phi\\left(\\frac{z^{i+1}-\\mu}{\\sigma_z}\\right) \\right) + \\mu $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute grid points for z\n",
    "z_grid = ((N * sigma_z * (norm.pdf((z_cutoffs[:-1] - mu) / sigma_z)\n",
    "                              - norm.pdf((z_cutoffs[1:] - mu) / sigma_z)))\n",
    "              + mu)\n",
    "print('Grid points = ', z_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our grid points.  Next, we need to determine the transitions between these grid points.  We want these transition probabilities to be such they they match the autocorrelation in the AR(1) process we are modeling using a first over Markov process.\n",
    "\n",
    "Define the probability of transitioning between grid point i and grid point j as: \n",
    "\n",
    "$$ \\pi_{i,j} = P\\left( z_{t} \\in \\left[z^{j},z^{j+1} \\right] | z_{t-1} \\in \\left[z^{i},z^{i+1} \\right] \\right) $$\n",
    "\n",
    "Using the fact that $\\varepsilon \\sim N(0,\\sigma_{\\varepsilon})$, we have:\n",
    "\n",
    "$$ \\pi_{i,j} = \\frac{N}{\\sqrt{2\\pi\\sigma_{z}^{2}}} \\int_{z^{i}}^{z^{i+1}} e^{-(\\varepsilon - \\mu)^{2}/(2\\sigma_{z}^{2})} \\left[ \\Phi\\left(\\frac{z^{j+1} - \\mu(1-\\rho) - \\rho\\mu}{\\sigma_{\\varepsilon}}\\right) - \\Phi\\left(\\frac{z^{j} - \\mu(1-\\rho) - \\rho\\mu}{\\sigma_{\\varepsilon}}\\right) \\right] d\\varepsilon $$\n",
    "\n",
    "To find each $\\pi_{i,j}$, we ned to evaluate this integral.  We'll do so numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "# define function that we will integrate\n",
    "def integrand(x, sigma_z, sigma_eps, rho, mu, z_j, z_jp1):\n",
    "    val = (np.exp((-1 * ((x - mu) ** 2)) / (2 * (sigma_z ** 2)))\n",
    "            * (norm.cdf((z_jp1 - (mu * (1 - rho)) - (rho * x)) / sigma_eps)\n",
    "               - norm.cdf((z_j - (mu * (1 - rho)) - (rho * x)) / sigma_eps)))\n",
    "    \n",
    "    return val\n",
    "\n",
    "# compute transition probabilities\n",
    "pi = np.empty((N, N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        results = integrate.quad(integrand, z_cutoffs[i], z_cutoffs[i + 1],\n",
    "                                 args = (sigma_z, sigma_eps, rho, mu,\n",
    "                                         z_cutoffs[j], z_cutoffs[j + 1]))\n",
    "        pi[i,j] = (N / np.sqrt(2 * np.pi * sigma_z ** 2)) * results[0]\n",
    "        \n",
    "# print('Transition matrix = ', pi)\n",
    "# print('pi sums = ', pi.sum(axis=0), pi.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now found \n",
    "\n",
    "$$ \\pi_{i,j} = P(z_{t}=z_{p}^{j}|z_{t-1}=z_{p}^{i}) $$\n",
    "\n",
    "Let's see how well this Markov process approximates the AR(1) above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simulate the Markov process - will make this a function so can call later\n",
    "def sim_markov(z_grid, pi, num_draws):\n",
    "    # draw some random numbers on [0, 1]\n",
    "    u = np.random.uniform(size=num_draws)\n",
    "\n",
    "    # Do simulations\n",
    "    z_discrete = np.empty(num_draws)  # this will be a vector of values \n",
    "    # we land on in the discretized grid for z\n",
    "    N = z_grid.shape[0]\n",
    "    oldind = int(np.ceil((N - 1) / 2)) # set initial value to median of grid\n",
    "    z_discrete[0] = z_grid[oldind]  \n",
    "    for i in range(1, num_draws):\n",
    "        sum_p = 0\n",
    "        ind = 0\n",
    "        while sum_p < u[i]:\n",
    "            sum_p = sum_p + pi[ind, oldind]\n",
    "#             print('inds =  ', ind, oldind)\n",
    "            ind += 1\n",
    "        if ind > 0:\n",
    "            ind -= 1\n",
    "        z_discrete[i] = z_grid[ind]\n",
    "        oldind = ind\n",
    "                            \n",
    "    return z_discrete\n",
    "\n",
    "\n",
    "# Call simulation function to get simulated values\n",
    "z_discrete = sim_markov(z_grid, np.transpose(pi), num_draws)\n",
    "                            \n",
    "                            \n",
    "# Plot AR(1) and Markov approximation\n",
    "sns.distplot(z_discrete, hist=True, kde=False, norm_hist=True)\n",
    "sns.kdeplot(np.array(z), bw=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.  Comparing methods\n",
    "    \n",
    "Working through the Adda-Cooper method was illustrative of how these approximations work.  Now let's compare across a few methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import module with Markov approximation methods\n",
    "import sys\n",
    "sys.path.append('../Code')\n",
    "import ar1_approx as ar1\n",
    "\n",
    "# Use Rouwenhorst (1995) method\n",
    "N = 11\n",
    "num_sigma = 4\n",
    "step = (num_sigma * sigma_z) / (N / 2)\n",
    "pi_R, z_grid_R = ar1.rouwen(rho, mu, step, N)\n",
    "z_discrete_R = sim_markov(z_grid_R, pi_R, num_draws)\n",
    "\n",
    "# Plot AR(1) and Markov approximation\n",
    "sns.distplot(z_discrete_R, hist=True, kde=False, norm_hist=True)\n",
    "sns.kdeplot(np.array(z), bw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Tauchen and Hussey (1991) method\n",
    "wgt = 0.5 + rho / 4\n",
    "baseSigma = wgt * sigma_eps + (1 - wgt) * sigma_z \n",
    "N_TH = 8\n",
    "z_grid_TH, pi_TH = ar1.tauchenhussey(N_TH, mu, rho, sigma_eps, baseSigma)\n",
    "z_discrete_TH = sim_markov(z_grid_TH[0], np.transpose(pi_TH), num_draws)\n",
    "\n",
    "# Plot AR(1) and Markov approximation\n",
    "sns.distplot(z_discrete_TH, hist=True, kde=False, norm_hist=True)\n",
    "sns.kdeplot(np.array(z), bw=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One caution here, if `N>8`, the results get a bit weird with represted values for in the $z$ grid and an asymetric distribution when simulated.  I'm not sure why grid points are repeated in thh $z$ grid, but I will look into this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Adda and Cooper (2003) method\n",
    "z_grid_AC, pi_AC = ar1.addacooper(N, mu, rho, sigma_eps)\n",
    "z_discrete_AC = sim_markov(z_grid_AC, np.transpose(pi_AC), num_draws)\n",
    "\n",
    "# Plot AR(1) and Markov approximation\n",
    "sns.distplot(z_discrete_AC, hist=True, kde=False, norm_hist=True)\n",
    "sns.kdeplot(np.array(z), bw=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three 3 plots are highly illustrative of the approaches these different methods take.  Rouwenhorst (1995) chooses and evenly spaced grid over $z$ and then fits the transition matrix over these points to produced the normal distribution found in the stationary distribution of $z$. Adda and Cooper (2003) break the the real line up into intervals with equal probability density and we see this in the density that results from the Markov process.  Tauchen and Hussey (1991) is somewhat between the other two methods in this repsect.  The grid for $z$ is not evenly spaced, but the probability density of any particular grid point varies in the stationary distribution.\n",
    "\n",
    "Also notice how the range of values in the $z$ grid changes across methods.  The Rouwenhorst (1995) and Tauchen and Hussey (1991) methods allow you to set this range with `step` and `baseSigma`, respectively.\n",
    "\n",
    "You can play around with `N`, $\\rho$, $\\sigma_{\\varepsilon}$, and see how these approximations perform.  For a rigorous comparison of these approaches, please see Kopecky and Suen (2010).  They find that the Rouwenhorst (1995) method outperforms the others when $\\rho$ is close to one.  You might consider the peristence and variability in the shocks to your autoregressive process when choosing the most appropriate method for your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try the Tauchen method from QuantEcon\n",
    "from quantecon import tauchen\n",
    "\n",
    "qe_result = tauchen(rho, sigma_eps, 3, N)\n",
    "pi_QE = qe_result.P\n",
    "z_grid_QE = np.linspace(-3*sigma_z, 3*sigma_z, N) # why can't this be returned from call to function??\n",
    "z_discrete_QE = sim_markov(z_grid_QE, np.transpose(pi_QE), num_draws)\n",
    "\n",
    "# Plot AR(1) and Markov approximation\n",
    "sns.distplot(z_discrete_QE, hist=True, kde=False, norm_hist=True)\n",
    "sns.kdeplot(np.array(z), bw=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks close to Rouwenhorst since it also uses an evenly spaced grid.  I'm not sure why the `quantecon.tauchen` method only returns the transition matrix... maybe becuase it's simply a linear spaced grid over the specied range of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. References\n",
    "* Adda, Jerome and Russell Cooper, *Dynamic Economics: Quantitative Methods and Applications*, MIT Press (2003).\n",
    "* Kopecky, Karen and Richard Suen, \"Finite State Markov-chain Approximations to Highly Persistent Processes\", *Review of Economic Dynamics*, 13:3, pp. 701-714 (July 2010)\n",
    "* Rouwenhorst, K Geert, \"Asset Pricing Implications of Equilibrium Business Cycle Models\", in *Frontiers of Business Cycle Research* (Thomas F. Cooley Ed.), Princeton University Press (1995).\n",
    "* Tauchen, George and Robert Hussey, \"Quadrature-Based Methods for Obtaining Approximate Solutions to Nonlinear Asset Pricing Models\", *Econometrica*, 59:2, pp. 371-396, (1991)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
