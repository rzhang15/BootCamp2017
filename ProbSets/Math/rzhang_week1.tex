\documentclass[letterpaper,12pt]{article}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{\footnotesize\textsl{OSM Lab, Summer 2017, Math PS \#1}}
\cfoot{}
\rfoot{\footnotesize\textsl{Page \thepage\ of \pageref{LastPage}}}
\renewcommand\headrulewidth{0pt}
\renewcommand\footrulewidth{0pt}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{dsfont}
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue,citecolor=red}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
%\numberwithin{equation}{section}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\newcommand\boldline{\arrayrulewidth{1pt}\hline}

\begin{document}

\title{Math Problem Set 1 \\
Open Source Macroeconomics Laboratory Boot Camp}
\author{Ruby Zhang}
\maketitle

\begin{enumerate}
  \item Exercises from the book:
    \begin{enumerate}
    \item[3.6]
    Since $A \in \mathscr{F}$, which is the power set of $\Omega$, then $A \subset \Omega = \cup_{i \in I} B_i$. Therefore, $A = \cup_{i \in I} A \cap B_i$. By the definition of a probability space, $P$ is countably additive on $\mathscr{F}$ so $P(\cup_{i \in I} A \cap B_i) = P(A) = \sum_{i \in I} P(A \cap B_i)$.
    \item[3.8]
    First, we will prove that if $E_1$ and $E_2$ are independent events, then $E_1^c$ and $E_2^c$ are also independent events. Note that:
      \begin{align*}
        P(E_1^c \cap E_2^c) &= P((E_1 \cup E_2)^c) \\
        & = 1 - (P(E_1) + P(E_2) - P(E_1 \cap E_2)) \\
        &= 1 - P(E_1) - P(E_2) + P(E_1) \times P(E_2) \\
        & = (1 - P(E_1))\times(1-P(E_2)) \\
        & = P(E_1^c) \times P(E_2^c)
      \end{align*}
    From the rule of unions and intersections, we have that $(\cup_{k=1}^n E_k)^c = \cap_{k=1}^n E_k^c$. In addition, since $\{E_k\}_{k=1}^n$ is a collection of independent events, so from the previous proof we have that $\{E_k^c\}_{k=1}^n$ is also a collection of independent events. Then we have that:
      \begin{align*}
        P(\cup_{k=1}^n E_k) &= 1 - P((\cup_{k=1}^n E_k)^c) \\
        &= 1 - P(\cap_{k=1}^n E_k^c) \\
        &= 1 - \prod_{k=1}^n P(E_k^c) \\
        &= 1 - \prod_{k=1}^n (1 - P(E_k))
      \end{align*}
    \item[3.11]
      From Bayes' Rule, we have that
      \begin{align*}
        P(s=\text{ crime}|s\text{ tested }+) &= \frac{P(s\text{ tested }+|s=\text{ crime})P(s=\text{ crime})}{P(s\text{ tested }+)} \\
        &= \frac{1 \times \frac{1}{250,000,000}}{\frac{1}{3,000,000}} \\
        &= \frac{3}{250}
      \end{align*}
    \item[3.12]
      Without loss of generality, suppose the contestant picked door $A_1$ and Monty opened door $A_2$, which contains a goat. We want to show that the contestant is better off picking door $A_3$. Since the contestant chose the first door with no prior information, then $P(A_1)=1/3$. Therefore, $P(A_2 \cup A_3)=2/3$. However, if $A_2$ contains a goat, then $P(A_3)=0$ and we know that $P(A_2 \cap A_3)$ since the car cannot be behind both. Thus, $P(A_2 \cup A_3)=P(A_3)=2/3$. The contestant would have double the chance of winning if they switched doors versus sticking with their original decision.

      In a similar situation with 10 doors, you would have a $1/10$ probability of winning if you stuck with your original decision, but you have a $9/10$ probability of winning if you switched to the remaining door.
    \item[3.16]
      We want to show that $E[(X-\mu)^2] = E[X^2] - \mu^2$. From the definition of variance and the fact that expectation is the weighted average and is thus additive, we have that:
        \begin{align*}
          Var[X] &= E[(X-\mu)^2] \\
          &= E[(X-E[X])^2] \\
          &= E[X^2 - 2XE[X] + E[X]^2] \\
          &= E[X^2] - E[2XE[X]] + E[E[X]^2] \\
          &= E[X^2] - 2E[X]^2 + E[X^2] \text{ since $E[X]^2$ is a constant} \\
          &= E[X^2] - E[X]^2 \\
          &= E[X^2] - \mu^2
        \end{align*}
    \item[3.33]
      For a binomial random variable $B$, we have that $B = \sum_{i=1}^n B_i$, where all $B_i$'s are independently Bernoulli distributed random variables such that $E[B_i] = p$ and $Var(B_i) = p(1-p)$. Thus, using the weak law of large numbers, we have that for all $\epsilon > 0$,
        \begin{equation*}
          P(|\frac{\sum_{i=1}^n B_i}{n} - p| \geq \epsilon) = P(|\frac{B}{n} - p| \geq \epsilon) \leq \frac{p(1-p)}{n\epsilon^2}
        \end{equation*}
    \item[3.36]
      Let $S_n = \sum_{i=1}^6242 X_i$, where $X_i$ is the probability that student $i$ will enroll at the school, so $X_i$'s are independently Bernoulli distributed random variables. Thus, $E[X_i]=\mu = 0.801$ and $Var[X_i] = 0.801 \times 0.199 = 0.1594$. We want to estimate $P(S_n \geq 5500) = 1 - P(S_n \leq 5500)$. Using the Central Limit Theorem, we have the estimation that:
        \begin{align*}
          P(\frac{S_n - n\mu}{\sigma\sqrt{n}} \leq y) &= P(S_n \leq \sigma\sqrt{n}y + n\mu) \\
          & \approx \frac{1}{\sqrt{2\pi}}\int_{-\infty}^y e^{\frac{-x^2}{2}}dx
        \end{align*}
      We can calculate $y$ as follows:
        \begin{align*}
          \sigma\sqrt{n}y + n\mu &= 5500 \\
          y &= \frac{5500 - 6242\times 0.801}{\sqrt{0.1594\times 6242}} \\
          &= 15.8563
        \end{align*}
      Computing the integral using wolfram alpha, we thus have that:
        \begin{equation*}
          P(S_n \geq 5500) = 1 - P(S_n \leq 5500) = 1 - 1 = 0
        \end{equation*}
    \end{enumerate}
  \item
    \begin{enumerate}
      \item
      \item
    \end{enumerate}
  \item
    To prove that Benson's Law is a well-defined discrete probability distribution, we must show that the probability of the entire space of outcomes is 1:
      \begin{align*}
        P(\Omega) &= \sum_{d = 1}^9 \log_{10} (1+\frac{1}{d}) \\
        &= \log_{10} (\sum_{d = 1}^9 \frac{d+1}{d}) \text{ this is a telescoping sum} \\
        &= \log_{10} 10 \\
        &= 1
      \end{align*}
  \item
    \begin{enumerate}
      \item
        The probability that the person wins $\$2^n$ is if they flip $n-1$ heads in a row then tails on the $n$th flip. The probability of that happening is $1/2^n$. Thus, for any given winning $x_n = \$2^n$, $p_nx_n = 1$. Since $n = \mathds{N}$, then $E[X] = \sum_{n = 1}^\infty p_nx_n = \sum_{n = 1}^\infty 1 = + \infty$.
      \item
        Since the player has log utility, for a given winning $x_n = \$2^n$, the utility $u_n = n\log{2}$. The probability has not changed, thus we have that $E[\log{X}] = \sum_{n = 1}^\infty p_nu_n = \log{2} \sum_{n = 1}^\infty \frac{n}{2^n} = 2\log{2}$, which was found using Wolfram Alpha.
    \end{enumerate}
  \item
  \item
    \begin{enumerate}
      \item
      \item
      \item
    \end{enumerate}
  \item
    \begin{enumerate}
      \item
      \item
      \item
      \item
      \item
    \end{enumerate}
  \item
  \item
    \begin{enumerate}
      \item
      \item
    \end{enumerate}
  \item

\end{enumerate}

\end{document}
