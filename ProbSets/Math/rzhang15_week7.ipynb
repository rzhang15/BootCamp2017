{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.3\n",
    "## Gradient Descent\n",
    "The basic idea of the gradient descient method is to find the direction of greatest increase of the function based on the magnitude of its gradient. The process iterates until convergence, with each iteration taking a step size in that direction. However, a drawback is the optimal step size is not often worth the cost and it takes many iterations to converge. This method can converge quickly for certain problems, but can be very slow for others (such as a function with many narrow canyons or troughs). However, it is good to use initially if the initial starting point is not close to the optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton and Quasi-Newton Methods\n",
    "Newton's method is a descent and local approximation method. It iterates by taking a step that's the size of the inverse of the Hessian in the direction of greatest increase (using its gradient at that point). If the function is a quadratic function, Newton's method can converge very rapidly. However, it is less appropriate to use if the starting point is far from the optimum, its Hessian is not positive definite, of the Hessian and inverse are expensive to compute. In order to circumvent the computational difficulty, quasi-Newton methods like BFGS can be used to approximate the Hessian (though worse convergence rate). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Gradient\n",
    "This method is between gradient descent and Newton's method by never computing nor storing Hessian approximations. The method moves along Q-conjugate directions. Conjugate gradient is best used for large quadratic optimization problems where the initial quandratic term matrix is symmetric, positive definite, and sparse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def steep_descent_quad(Q,b,x0,tol=1e-6,maxiters=200):\n",
    "    for k in range(maxiters):\n",
    "        Df = Q@x0-b\n",
    "        if np.linalg.norm(Df)<tol:\n",
    "            converge=True\n",
    "            break\n",
    "        converge=False\n",
    "        alpha = Df.T@Df/((Df.T@Q)@Df)\n",
    "        x1 = x0-alpha*Df\n",
    "        x0=x1\n",
    "    return x1,converge,k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.75  9.5   9.25] True 47\n"
     ]
    }
   ],
   "source": [
    "Q = np.array([[2,-1,0],[-1,2,-1],[0,-1,2]])\n",
    "b = np.array([2,4,9])\n",
    "x0 = np.array([1,1,1])\n",
    "result, convergence, iterations= steep_descent_quad(Q,b,x0)\n",
    "result = np.round(result,decimals=3)\n",
    "print(result,convergence,iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_forward(f,x,rerr):\n",
    "    n = x.shape[0]\n",
    "    m = f(x).shape\n",
    "    if len(m)==0:\n",
    "        m = 1\n",
    "    h = 2*np.sqrt(rerr)\n",
    "    Df = np.zeros((m,n))\n",
    "    for i in range(n):\n",
    "        ei = np.zeros(n)\n",
    "        ei[i] = 1\n",
    "        Df[:,i] = (f(x+h*ei)-f(x))*(1/h)\n",
    "    return Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation: [[-0.998 -3.998 -7.998]] Actual: [-1 -4 -8]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: 0.5*x@(Q@x)-b@x+3 \n",
    "Df_approx = df_forward(f,x0,1e-6)\n",
    "print('Approximation:', Df_approx, 'Actual:', Q@x0-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^TQ\\mathbf{x}-\\mathbf{b}^T\\mathbf{x}$, then we have that the derivative of $f$ is $Df = Q\\mathbf{x}-\\mathbf{b}$. Then the minimum is found at $Q\\mathbf{x^*}-\\mathbf{b} = 0 \\Longrightarrow Q\\mathbf{x^*} = \\mathbf{b}$. If we take the Hessian, we have that $D^2f = Q$. From Newton's method and an initial starting point, we have that $\\mathbf{x_1} = \\mathbf{x_0}-Q^{-1}Df(\\mathbf{x_0}) \\Longrightarrow Q\\mathbf{x_1} = Df(\\mathbf{x_0})-Q\\mathbf{x_0}$. Note that $Df = Q\\mathbf{x}-\\mathbf{b}$ for all $\\mathbf{x}$. Thus, $Q\\mathbf{x_1} = \\mathbf{b} = Q\\mathbf{x^*}$. Thus, $\\mathbf{x_1}=\\mathbf{x^*}$ so Newton's method reaches the optimal for quadratic problems in a single iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
